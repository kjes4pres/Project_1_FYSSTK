{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4585ff12",
   "metadata": {},
   "source": [
    "# Task a) and b) of Project 1 - FYS-STK4155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aada930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,       \n",
    "    \"font.family\": \"serif\",    \n",
    "    \"font.size\": 10, \n",
    "})\n",
    "\n",
    "import matplotlib.style as mplstyle\n",
    "mplstyle.use(['ggplot', 'fast'])\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b471adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Colormap for plotting\n",
    "colormap = 'plasma'\n",
    "\n",
    "# Size of test dataset\n",
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3bb51",
   "metadata": {},
   "source": [
    "# a) Ordinary Least Squares (OLS)\n",
    "* Using OLS to predict the Runge function.\n",
    "* Method is applied for the different number of data points in `n_vals` and for the different polynomial degrees in `p_vals`.\n",
    "* Uses sklearn functionalities to make the model and assess the MSE and R^2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036adc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vals = np.arange(50, 1050, 50)  # Range of number of data points, 50-1000\n",
    "p_vals = np.arange(2, 16)  # Range of polynomial degrees, 2-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test, full) = make_dataset(n_points)\n",
    "x_train, y_train = train\n",
    "x_test, y_test = test\n",
    "x_all, y_all, y_all_clean = full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403dc85c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m results = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m n_vals:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     x, y = make_data(n)  \u001b[38;5;66;03m# making a dataset with size n\u001b[39;00m\n\u001b[32m      5\u001b[39m     x = x.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# splitting the data into train and test data sets\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for n in n_vals:\n",
    "    train, test, full = make_dataset(n)  # making a dataset with size n\n",
    "    x_train, y_train = train\n",
    "    x_test, y_test = test\n",
    "    x_all, y_all, y_all_clean = full\n",
    "\n",
    "    x = x.reshape(-1, 1)\n",
    "\n",
    "    # splitting the data into train and test data sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # making an OLS model for a given polynomial degree, p\n",
    "    for p in p_vals:\n",
    "        model = make_pipeline(\n",
    "            PolynomialFeatures(degree=p, include_bias=False),\n",
    "            StandardScaler(with_mean=False),\n",
    "            LinearRegression(fit_intercept=False)\n",
    "        )\n",
    "        \n",
    "        # using the training data to train the model\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        # using the test data to make a prediction, unsee data for the model\n",
    "        y_pred = model.predict(x_test)\n",
    "        \n",
    "        # assessing the model with scores\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # extracting the model features\n",
    "        theta = model.named_steps['linearregression'].coef_\n",
    "        \n",
    "        # saving the results in a pandas dataframe\n",
    "        results.append({\n",
    "            'n': n,\n",
    "            'p': p,\n",
    "            'theta': theta,\n",
    "            'MSE': mse,\n",
    "            'R2': r2\n",
    "        })\n",
    "\n",
    "df_OLS = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21646237",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "num_colors = len(n_vals)\n",
    "cmap = plt.get_cmap(colormap, num_colors)\n",
    "\n",
    "for i, en in enumerate(n_vals):\n",
    "    n_df = df_OLS[df_OLS['n'] == en]\n",
    "    color = cmap(i) \n",
    "    ax[0].plot(n_df['p'], n_df['MSE'], marker='o', markersize='3', linewidth='2', color=color, label=f'n: {en}')\n",
    "\n",
    "ax[0].set_title('MSE as a function of polynomial degree')\n",
    "ax[0].legend(loc='upper left')\n",
    "ax[0].set_xlabel('Polynomial degree')\n",
    "ax[0].set_ylabel('MSE')\n",
    "\n",
    "for i, en in enumerate(n_vals):\n",
    "    n_df = df_OLS[df_OLS['n'] == en]\n",
    "    color = cmap(i) \n",
    "    ax[1].plot(n_df['p'], n_df['R2'], marker='o', markersize='3', linewidth='2', color=color, label=f'n: {en}')\n",
    "\n",
    "ax[1].set_title(r'$R^2$ as a function of polynomial degree')\n",
    "ax[1].legend(loc='upper left')\n",
    "ax[1].set_xlabel('Polynomial degree')\n",
    "ax[1].set_ylabel(r'$R^2$')\n",
    "\n",
    "fig.suptitle('OLS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702505c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "num_colors = len(p_vals)\n",
    "cmap = plt.get_cmap(colormap, num_colors)\n",
    "\n",
    "for i, pe in enumerate(p_vals):\n",
    "    p_df = df_OLS[df_OLS['p'] == pe]\n",
    "    color = cmap(i) \n",
    "    ax[0].plot(p_df['n'], p_df['MSE'], marker='o', markersize='3', linewidth='2', color=color, label=f'p: {pe}')\n",
    "\n",
    "ax[0].set_title('MSE as a function of number of datapoints')\n",
    "ax[0].legend(loc='upper right')\n",
    "ax[0].set_xlabel('Number of datapoints')\n",
    "ax[0].set_ylabel('MSE')\n",
    "\n",
    "for i, pe in enumerate(p_vals):\n",
    "    p_df = df_OLS[df_OLS['p'] == pe]\n",
    "    color = cmap(i) \n",
    "    ax[1].plot(p_df['n'], p_df['R2'], marker='o', markersize='3', linewidth='2', color=color, label=f'p: {pe}')\n",
    "\n",
    "ax[1].set_title(r'$R^2$ as a function of number of datapoints')\n",
    "ax[1].legend(loc='upper right')\n",
    "ax[1].set_xlabel('Number of datapoints')\n",
    "ax[1].set_ylabel(r'$R^2$')\n",
    "\n",
    "fig.suptitle('OLS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c8f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "num_colors = len(n_vals)\n",
    "cmap = plt.get_cmap(colormap, num_colors)\n",
    "\n",
    "for i, en in enumerate(n_vals):\n",
    "    n_df = df_OLS[df_OLS['n'] == en]\n",
    "    color = cmap(i) \n",
    "    ax[0].plot(n_df['p'], n_df['theta'].apply(lambda x: x[0]), marker='o', markersize='3', linewidth='2', color=color, label=f'N: {en}')\n",
    "    ax[1].plot(n_df['p'], n_df['theta'].apply(lambda x: x[1]), marker='o', markersize='3', linewidth='2', color=color, label=f'N: {en}')\n",
    "\n",
    "ax[0].set_title(r'$\\theta_1$')\n",
    "ax[1].set_title(r'$\\theta_2$')\n",
    "\n",
    "fig.suptitle(f'Features as a function of polynomial degree \\n OLS', y=1.05)\n",
    "\n",
    "for axs in ax:\n",
    "    axs.legend(loc='upper left')\n",
    "    axs.set_xlabel('Polynomial degree')\n",
    "    axs.set_ylabel(r'$\\theta$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10245345",
   "metadata": {},
   "source": [
    "# b) Ridge regression\n",
    "* Using Ridge regression to predict the Runge function.\n",
    "* Method is applied for the different number of data points in `n_vals` and for the different polynomial degrees in `p_vals`, and for different values of the penalization parameter $\\lambda$.\n",
    "* Uses sklearn functionalities to make the model and assess the MSE and R^2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59186026",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-8, 2, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b40ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for n in n_vals:\n",
    "    x, y = make_data(n)\n",
    "    x = x.reshape(-1, 1)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=test_size, random_state=2018\n",
    "    )\n",
    "\n",
    "    for p in p_vals:\n",
    "        for l in lambdas:\n",
    "            model = make_pipeline(\n",
    "                PolynomialFeatures(degree=p, include_bias=False),\n",
    "                StandardScaler(with_mean=False),\n",
    "                Ridge(alpha=l, fit_intercept=False)\n",
    "            )\n",
    "        \n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred = model.predict(x_test)\n",
    "\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "            theta = model.named_steps['ridge'].coef_\n",
    "\n",
    "            results.append({\n",
    "                'n': n,\n",
    "                'p': p,\n",
    "                'lambda': l, \n",
    "                'theta': theta,\n",
    "                'MSE': mse,\n",
    "                'R2': r2\n",
    "            })\n",
    "\n",
    "df_Ridge = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing a n-value to plot for\n",
    "n_val = n_vals[-1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "colormap='plasma'\n",
    "num_colors = len(lambdas)\n",
    "cmap = plt.get_cmap(colormap, num_colors)\n",
    "\n",
    "for i, l in enumerate(lambdas):\n",
    "    n_df = df_Ridge[(df_Ridge['lambda'] == l) & (df_Ridge['n'] == n_val)]\n",
    "    color = cmap(i) \n",
    "    ax[0].plot(n_df['p'], n_df['MSE'], marker='o', markersize='3', linewidth='2', color=color, label=f'L: {l:.2e}')\n",
    "    ax[1].plot(n_df['p'], n_df['R2'], marker='o', markersize='3', linewidth='2', color=color, label=f'L: {l:.2e}')\n",
    "\n",
    "ax[0].set_title('MSE as a function of polynomial degree', fontsize=10)\n",
    "ax[0].legend(loc='upper right', fontsize=8)\n",
    "ax[0].set_xlabel('Polynomial degree')\n",
    "ax[0].set_ylabel('MSE')\n",
    "\n",
    "ax[1].set_title(r'$R^2$ as a function of polynomial degree', fontsize=10)\n",
    "ax[1].legend(loc='upper right', fontsize=8)\n",
    "ax[1].set_xlabel('Polynomial degree')\n",
    "ax[1].set_ylabel(r'$R^2$')\n",
    "\n",
    "fig.suptitle(f'Ridge \\n Datapoints: {n_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ea689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing a n-value to plot for\n",
    "n_val = n_vals[-1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "colormap='plasma'\n",
    "num_colors = len(p_vals)\n",
    "cmap = plt.get_cmap(colormap, num_colors)\n",
    "\n",
    "for i, pe in enumerate(p_vals):\n",
    "    n_df = n_df = df_Ridge[(df_Ridge['p'] == pe) & (df_Ridge['n'] == n_val)]\n",
    "    color = cmap(i) \n",
    "    ax[0].plot(n_df['lambda'], n_df['MSE'], marker='o', markersize='3', linewidth='2', color=color, label=f'p: {pe:.0f}')\n",
    "    ax[1].plot(n_df['lambda'], n_df['R2'], marker='o', markersize='3', linewidth='2', color=color, label=f'p: {pe:.0f}')\n",
    "\n",
    "ax[0].set_title('MSE as a function of hyperparameter', fontsize=10)\n",
    "ax[0].legend(loc='upper right', fontsize=8)\n",
    "ax[0].set_xlabel(r'$\\lambda$')\n",
    "ax[0].set_ylabel('MSE')\n",
    "\n",
    "ax[1].set_title(r'$R^2$ as a function of hyperparameter', fontsize=10)\n",
    "ax[1].legend(loc='upper right', fontsize=8)\n",
    "ax[1].set_xlabel(r'$\\lambda$')\n",
    "ax[1].set_ylabel(r'$R^2$')\n",
    "\n",
    "fig.suptitle(f'Ridge \\n Datapoints: {n_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03040daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting specific lambda values to plot\n",
    "lambda_subset = [lambdas[3], lambdas[6], lambdas[9], lambdas[13]]\n",
    "\n",
    "fig, ax = plt.subplots(4, 2, figsize=(12, 8), sharex='col')\n",
    "colormap = 'plasma'\n",
    "num_colors = len(n_vals)\n",
    "cmap = plt.get_cmap(colormap, num_colors)\n",
    "\n",
    "for j, l_val in enumerate(lambda_subset):\n",
    "    for i, en in enumerate(n_vals):\n",
    "        n_df = df_Ridge[(df_Ridge['n'] == en) & (df_Ridge['lambda'] == l_val)]\n",
    "        color = cmap(i)\n",
    "        ax[j, 0].plot(\n",
    "            n_df['p'], n_df['theta'].apply(lambda x: x[0]),\n",
    "            marker='o', markersize=3, linewidth=1.5, color=color\n",
    "        )\n",
    "        ax[j, 1].plot(\n",
    "            n_df['p'], n_df['theta'].apply(lambda x: x[1]),\n",
    "            marker='o', markersize=3, linewidth=1.5, color=color\n",
    "        )\n",
    "\n",
    "    ax[j, 0].text(\n",
    "        0.02, 0.95, rf'$\\lambda$={l_val:.4f}',\n",
    "        transform=ax[j, 0].transAxes,\n",
    "        fontsize=8, va='top', ha='left',\n",
    "        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, pad=1)\n",
    "    )\n",
    "    ax[j, 1].text(\n",
    "        0.02, 0.95, rf'$\\lambda$={l_val:.4f}',\n",
    "        transform=ax[j, 1].transAxes,\n",
    "        fontsize=8, va='top', ha='left',\n",
    "        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, pad=1)\n",
    "    )\n",
    "\n",
    "fig.suptitle('Features as a function of Polynomial Degree \\n Ridge regression', y=0.95)\n",
    "\n",
    "for axs in ax[:, 0]:\n",
    "    axs.set_ylabel(r'$\\theta$')\n",
    "\n",
    "ax[0, 0].set_title(r'$\\theta_1$')\n",
    "ax[0, 1].set_title(r'$\\theta_2$')\n",
    "\n",
    "ax[3, 0].set_xlabel('Polynomial Degree')\n",
    "ax[3, 1].set_xlabel('Polynomial Degree')\n",
    "\n",
    "# Colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min(n_vals), vmax=max(n_vals)))\n",
    "sm.set_array([])\n",
    "\n",
    "cbar = fig.colorbar(\n",
    "    sm, ax=ax.ravel().tolist(), orientation='vertical',\n",
    "    fraction=0.05, pad=0.05, location='right'\n",
    ")\n",
    "cbar.set_label('Number of datapoints values')\n",
    "\n",
    "#plt.tight_layout(rect=[0, 0.05, 1, 0.96])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
