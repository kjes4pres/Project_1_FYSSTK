{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbfd11b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.style as mplstyle\n",
    "\n",
    "from funcs import *\n",
    "\n",
    "mplstyle.use(['ggplot', 'fast'])\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93d240",
   "metadata": {},
   "source": [
    "#### Part c) Writing your own gradient descent code\n",
    "\n",
    "* Replace analytical expressions for the optimal parameters with gradient descent\n",
    "* Fixed learning rate\n",
    "* Study and compare results to a) and b).\n",
    "* Discuss the role of the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62797f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, eta=0.1, n_iters=1000, lamb=0):\n",
    "    ''' \n",
    "    Lambda = 0 -> OLS\n",
    "    '''\n",
    "\n",
    "    tol = 1e-8\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    theta = np.random.randn(n_features, 1)  # First guess\n",
    "\n",
    "    mse_history = np.zeros(n_iters)  # For storing MSE\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        grad = (2/n_samples) * X.T @ (X @ theta - y) + 2 * lamb * theta\n",
    "        new_theta = theta - eta*grad\n",
    "        if np.linalg.norm(new_theta - theta) < tol:\n",
    "            print(f'Breaking loop at iteration number: {i}')\n",
    "        theta = new_theta\n",
    "\n",
    "        mse = MSE(y_data=y, y_pred=X @ theta)\n",
    "        mse_history[i] = mse\n",
    "    \n",
    "    return theta, mse_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa8de92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen parameters for GD analysis\n",
    "p_vals = 5  # polynomial degree\n",
    "lamb = [0.01, 1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e8bda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for OLS\n",
    "# 10 data points\n",
    "x, y = make_data(n=10)\n",
    "X = polynomial_features(x, p_vals, intercept=True)\n",
    "X, y = standardize(X, y)\n",
    "X_train, X_test, y_train, y_test = split_n_train(X, y, size=0.2)\n",
    "gd_OLS_theta_n10, gd_OLS_mse_n10 = gradient_descent(X_train, y_train)\n",
    "\n",
    "# 100 data points\n",
    "x, y = make_data(n=100)\n",
    "X = polynomial_features(x, p_vals, intercept=True)\n",
    "X, y = standardize(X, y)\n",
    "X_train, X_test, y_train, y_test = split_n_train(X, y, size=0.2)\n",
    "gd_OLS_theta_n100, gd_OLS_mse_n100 = gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eec5ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for Ridge\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
