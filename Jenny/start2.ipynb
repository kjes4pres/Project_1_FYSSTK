{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Part a: Ordinary Least Squares (OLS) for the Runge function}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Runges function: f(x) = 1 / ( 1 + 25x^2), x in [-1, 1]\n",
    "\n",
    "* Create dataset for f(x)\n",
    "* Add stochastic noise\n",
    "\n",
    "* Perform standard OLS regression using polynomials in x up to order 15 or higher \n",
    "* Explore the dependence on the number of data points and the polynomial degree\n",
    "\n",
    "* Evaluate MSE and the $R^2$ score function \n",
    "\n",
    "* Plot MSE and $R^2$ as functions of polynomial degree \n",
    "* Plot also the parameters $\\theta$ as you increase order of polynomial \n",
    "\n",
    "Has to include:\n",
    "* Scaling/centering of data \n",
    "* Split of the data in training and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/jennyguldvog/Project_1_FYSSTK/Jenny/start2.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jennyguldvog/Project_1_FYSSTK/Jenny/start2.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnumpy\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jennyguldvog/Project_1_FYSSTK/Jenny/start2.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jennyguldvog/Project_1_FYSSTK/Jenny/start2.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jennyguldvog/Project_1_FYSSTK/Jenny/start2.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jennyguldvog/Project_1_FYSSTK/Jenny/start2.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpandas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def polynomial_features(x, p, intercept=False):\n",
    "    n = len(x)\n",
    "    if intercept:\n",
    "        X = np.zeros((n, p+1))\n",
    "        X[:, 0] = 1   # intercept-kolonnen\n",
    "        for i in range(1, p+1):   \n",
    "            X[:, i] = x**i\n",
    "    else:\n",
    "        X = np.zeros((n, p))\n",
    "        for i in range(p):\n",
    "            X[:, i] = x**(i+1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "datapoints = [100, 1000, 10000, 100000]\n",
    "poly = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datapoints' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jennyguldvog/Project_1_FYSSTK/Jenny/start2.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jennyguldvog/Project_1_FYSSTK/Jenny/start2.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results_ols \u001b[39m=\u001b[39m {}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jennyguldvog/Project_1_FYSSTK/Jenny/start2.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m rng \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mdefault_rng(\u001b[39m73\u001b[39m) \u001b[39m#random number generator med random seed = 73\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jennyguldvog/Project_1_FYSSTK/Jenny/start2.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m datapoints:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jennyguldvog/Project_1_FYSSTK/Jenny/start2.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,n)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jennyguldvog/Project_1_FYSSTK/Jenny/start2.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39m25\u001b[39m\u001b[39m*\u001b[39mx\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datapoints' is not defined"
     ]
    }
   ],
   "source": [
    "results_ols = {}\n",
    "\n",
    "rng = np.random.default_rng(73) #random number generator med random seed = 73\n",
    "\n",
    "for n in datapoints:\n",
    "    \n",
    "    x = np.linspace(-1,1,n)\n",
    "    y = 1 / (1 + 25*x**2)\n",
    "    #noise = 0.5 * np.random.randn(n)    # Gaussian (normal distribution) noise\n",
    "    #y = y + noise\n",
    "\n",
    "    noise = 0.5 * rng.standard_normal(n)\n",
    "\n",
    "    y = y + noise \n",
    "\n",
    "    #Make one index-split per n, and reuse for all p\n",
    "    idx = np.arange(n)\n",
    "    idx_train, idx_test = train_test_split(idx, test_size=0.2, random_state=73)\n",
    "\n",
    "\n",
    "    for p in poly:\n",
    "        \n",
    "        Xp = polynomial_features(x, p, intercept=False)\n",
    "\n",
    "        #use the same indexes for Xp and y\n",
    "        X_train = Xp[idx_train, :]\n",
    "        X_test = Xp[idx_test, :]\n",
    "        y_train = y[idx_train]\n",
    "        y_test = y[idx_test]\n",
    "\n",
    "        #scale only on train, and then use the same scaler for test \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_s = scaler.transform(X_train)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "\n",
    "        #center y with train mean (and same offset on test)\n",
    "        y_offset = np.mean(y_train)\n",
    "        y_train_s = y_train - y_offset\n",
    "        y_test_s = y_test - y_offset\n",
    "\n",
    "        #calculate theta\n",
    "        theta = np.linalg.pinv(X_train_s) @ y_train_s #more stable even when X^TX er d√•rlig kondisjonert eller p er stor\n",
    "\n",
    "        #predicting y with the model (with sentered y)\n",
    "        y_hat_train_sc = X_train_s @ theta\n",
    "        y_hat_test_sc = X_test_s @ theta\n",
    "\n",
    "        #unnormalising y - to be on the same scale as the original y values \n",
    "        #y_hat_train = y_hat_train_sc + y_offset\n",
    "        #y_hat_test = y_hat_test_sc + y_offset\n",
    "\n",
    "        #calculate MSE \n",
    "        mse_train = np.mean((y_train_s - y_hat_train_sc)**2)\n",
    "        mse_test = np.mean((y_test_s - y_hat_test_sc)**2)\n",
    "\n",
    "        #calculate R^2\n",
    "        SSE_tr = np.sum((y_train_s - y_hat_train_sc)**2)\n",
    "        SST_tr = np.sum((y_train_s - y_train_s.mean())**2)\n",
    "        #SST_tr = np.sum(y_train_s**2)\n",
    "        R2_train = 1 - SSE_tr / SST_tr\n",
    "\n",
    "        SSE_te = np.sum((y_test_s - y_hat_test_sc)**2)\n",
    "        SST_te = np.sum((y_test_s - y_train_s.mean())**2)\n",
    "        #SST_te = np.sum(y_test_s**2)\n",
    "        R2_test = 1 - SSE_te / SST_te\n",
    "\n",
    "        #sjekke stabilitet \n",
    "\n",
    "        theta_norm = np.linalg.norm(theta)\n",
    "\n",
    "        results_ols[(n, p)] = {\n",
    "            #train, test, scaler values\n",
    "            \"X_train\": X_train_s, \"X_test\": X_test_s,\n",
    "            \"y_train\": y_train_s, \"y_test\": y_test_s,\n",
    "            \"scaler_mean\": scaler.mean_, \"scaler_scale\": scaler.scale_,\n",
    "            \"y_offset\": y_offset,\n",
    "            #index and x values\n",
    "            \"idx_train\": idx_train, \"idx_test\": idx_test,\n",
    "            \"x_train\": x[idx_train], \"x_test\": x[idx_test],\n",
    "            #theta, mse and R^2 values\n",
    "            \"theta\": theta, \n",
    "            \"mse_train\": mse_train, \"mse_test\": mse_test,\n",
    "            \"R2_train\": R2_train, \"R2_test\": R2_test, \n",
    "            #theta norm \n",
    "            \"theta_norm\": theta_norm,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#plotting for a)\n",
    "\n",
    "#plot MSE test\n",
    "ns = sorted({n for (n, _) in results_ols.keys()})\n",
    "plt.figure()\n",
    "for n_focus in ns:\n",
    "    ps = sorted({p for (n_key, p) in results_ols.keys() if n_key == n_focus})\n",
    "    mse_te_ols = [results_ols[(n_focus, p)][\"mse_test\"] for p in ps]\n",
    "    plt.plot(ps, mse_te_ols, marker=\"o\", label=f\"n={n_focus}\")\n",
    "plt.title(\"Test MSE - OLS\")\n",
    "plt.xlabel(\"Polynomial degree p\")\n",
    "plt.ylabel(\"Test MSE\")\n",
    "plt.legend(title=\"Datapoints\")\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "#plot MSE train\n",
    "ns = sorted({n for (n, _) in results_ols.keys()})\n",
    "plt.figure()\n",
    "for n_focus in ns:\n",
    "    ps = sorted({p for (n_key, p) in results_ols.keys() if n_key == n_focus})\n",
    "    mse_tr_ols = [results_ols[(n_focus, p)][\"mse_train\"] for p in ps]\n",
    "    plt.plot(ps, mse_tr_ols, marker=\"o\", label=f\"n={n_focus}\")\n",
    "plt.title(\"Train MSE - OLS\")\n",
    "plt.xlabel(\"Polynomial degree p\")\n",
    "plt.ylabel(\"Train MSE\")\n",
    "plt.legend(title=\"Datapoints\")\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "#plot R^2 test\n",
    "ns = sorted({n for (n, _) in results_ols.keys()})\n",
    "plt.figure()\n",
    "for n_focus in ns:\n",
    "    ps = sorted({p for (n_key, p) in results_ols.keys() if n_key == n_focus})\n",
    "    R2_te_ols = [results_ols[(n_focus, p)][\"R2_test\"] for p in ps]\n",
    "    plt.plot(ps, R2_te_ols, marker=\"o\", label=f\"n={n_focus}\")\n",
    "plt.title(\"R^2 test - OLS\")\n",
    "plt.xlabel(\"Polynomial degree p\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.legend(title=\"Datapoints\")\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "#plot R^2 train\n",
    "ns = sorted({n for (n, _) in results_ols.keys()})\n",
    "plt.figure()\n",
    "for n_focus in ns:\n",
    "    ps = sorted({p for (n_key, p) in results_ols.keys() if n_key == n_focus})\n",
    "    R2_tr_ols = [results_ols[(n_focus, p)][\"R2_train\"] for p in ps]\n",
    "    plt.plot(ps, R2_tr_ols, marker=\"o\", label=f\"n={n_focus}\")\n",
    "plt.title(\"R^2 train - OLS\")\n",
    "plt.xlabel(\"Polynomial degree p\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.legend(title=\"Datapoints\")\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "#plot theta\n",
    "#m√• finne ut hva beste m√•te for det er "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-MSE:\n",
    "* For sm√• n, (n = 100), blir test-MSE st√∏rre n√•r p blir st√∏rre, $\\to$ overfitting (eller mer varians?)\n",
    "* For st√∏rre n (n = 1000, 10000, 100000) faller MSE n√•r p √∏ker og flater ut, rundt 0.25 (n√¶r st√∏yvariansen). Dette er den irreducerbare feilen: selv med en perfekt modell kan vi ikke komme mye under  $\\sigma^2$ i forventning.\n",
    "\n",
    "* $\\sigma^2 = (0.5)^2 = 0.25$. \n",
    "\n",
    "Train-MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Part b: Adding Ridge regression for the Runge function}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def Ridge_parameters(X, y, lam):\n",
    "    # Assumes X is scaled and has no intercept column\n",
    "    n, p = X.shape\n",
    "    I = np.eye(p)\n",
    "    return np.linalg.inv(X.T @ X + lam* I) @ X.T @ y\n",
    "\n",
    "lams = [0.0] + list(np.logspace(-6, 3, 20))  # 1e-6 ... 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "results_ridge = {}\n",
    "\n",
    "rng = np.random.default_rng(73) #random number generator med random seed = 73\n",
    "\n",
    "for n in datapoints:\n",
    "    \n",
    "    x = np.linspace(-1,1,n)\n",
    "    y = 1 / (1 + 25*x**2)\n",
    "    #noise = 0.5 * np.random.randn(n)    # Gaussian (normal distribution) noise\n",
    "    #y = y + noise\n",
    "\n",
    "    noise = 0.5 * rng.standard_normal(n)\n",
    "\n",
    "    y = y + noise \n",
    "\n",
    "    #Make one index-split per n, and reuse for all p\n",
    "    idx = np.arange(n)\n",
    "    idx_train, idx_test = train_test_split(idx, test_size=0.2, random_state=73)\n",
    "\n",
    "\n",
    "    for p in poly:\n",
    "        \n",
    "        Xp = polynomial_features(x, p, intercept=False)\n",
    "\n",
    "        #use the same indexes for Xp and y\n",
    "        X_train = Xp[idx_train, :]\n",
    "        X_test = Xp[idx_test, :]\n",
    "        y_train = y[idx_train]\n",
    "        y_test = y[idx_test]\n",
    "\n",
    "        #scale only on train, and then use the same scaler for test \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_s = scaler.transform(X_train)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "\n",
    "        #center y with train mean (and same offset on test)\n",
    "        y_offset = np.mean(y_train)\n",
    "        y_train_s = y_train - y_offset\n",
    "        y_test_s = y_test - y_offset\n",
    "\n",
    "        for lam in lams:\n",
    "            theta = Ridge_parameters(X_train_s, y_train_s, lam)\n",
    "\n",
    "            #predicting y with the model (with sentered y)\n",
    "            y_hat_train_sc = X_train_s @ theta\n",
    "            y_hat_test_sc = X_test_s @ theta\n",
    "\n",
    "            #calculate MSE \n",
    "            mse_train = np.mean((y_train_s - y_hat_train_sc)**2)\n",
    "            mse_test = np.mean((y_test_s - y_hat_test_sc)**2)\n",
    "\n",
    "            #calculate R^2\n",
    "            SSE_tr = np.sum((y_train_s - y_hat_train_sc)**2)\n",
    "            SST_tr = np.sum((y_train_s - y_train_s.mean())**2)\n",
    "            #SST_tr = np.sum(y_train_s**2)\n",
    "            R2_train = 1 - SSE_tr / SST_tr\n",
    "\n",
    "            SSE_te = np.sum((y_test_s - y_hat_test_sc)**2)\n",
    "            SST_te = np.sum((y_test_s - y_train_s.mean())**2)\n",
    "            #SST_te = np.sum(y_test_s**2)\n",
    "            R2_test = 1 - SSE_te / SST_te\n",
    "\n",
    "            #sjekke stabilitet \n",
    "\n",
    "            theta_norm = np.linalg.norm(theta)\n",
    "\n",
    "            # ---------- LAGRE PER (n, p, modell, lam) ----------\n",
    "            key = (n, p, \"ridge\", float(lam))\n",
    "            results_ridge[key] = {\n",
    "            \"theta\": theta,\n",
    "            \"theta_norm\": theta_norm,\n",
    "            \"mse_train\": mse_train, \"mse_test\": mse_test,\n",
    "            \"R2_train\": R2_train,   \"R2_test\": R2_test,\n",
    "            \"lam\": float(lam), \"reg_label\": \"ridge\",\n",
    "            \"scaler_mean\": scaler.mean_, \"scaler_scale\": scaler.scale_,\n",
    "            \"y_offset\": y_offset,\n",
    "            \"idx_train\": idx_train, \"idx_test\": idx_test,\n",
    "            \"x_train\": x[idx_train], \"x_test\": x[idx_test],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#plotting for b)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#plot MSE test\n",
    "ns = sorted({n for (n, _) in results_ridge.keys()})\n",
    "plt.figure()\n",
    "for n_focus in ns:\n",
    "    ps = sorted({p for (n_key, p) in results_ridge.keys() if n_key == n_focus})\n",
    "    mse_te_ridge = [results_ridge[(n_focus, p)][\"mse_test\"] for p in ps]\n",
    "    plt.plot(ps, mse_te_ridge, marker=\"o\", label=f\"n={n_focus}\")\n",
    "plt.title(\"Test MSE - Ridge\")\n",
    "plt.xlabel(\"Polynomial degree p\")\n",
    "plt.ylabel(\"Test MSE\")\n",
    "plt.legend(title=\"Datapoints\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#plot MSE train\n",
    "ns = sorted({n for (n, _) in results_ridge.keys()})\n",
    "plt.figure()\n",
    "for n_focus in ns:\n",
    "    ps = sorted({p for (n_key, p) in results_ridge.keys() if n_key == n_focus})\n",
    "    mse_tr_ridge = [results_ridge[(n_focus, p)][\"mse_train\"] for p in ps]\n",
    "    plt.plot(ps, mse_tr_ridge, marker=\"o\", label=f\"n={n_focus}\")\n",
    "plt.title(\"Train MSE - Ridge\")\n",
    "plt.xlabel(\"Polynomial degree p\")\n",
    "plt.ylabel(\"Train MSE\")\n",
    "plt.legend(title=\"Datapoints\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#plot R^2 test\n",
    "ns = sorted({n for (n, _) in results_ridge.keys()})\n",
    "plt.figure()\n",
    "for n_focus in ns:\n",
    "    ps = sorted({p for (n_key, p) in results_ridge.keys() if n_key == n_focus})\n",
    "    R2_te_ridge = [results_ridge[(n_focus, p)][\"R2_test\"] for p in ps]\n",
    "    plt.plot(ps, R2_te_ridge, marker=\"o\", label=f\"n={n_focus}\")\n",
    "plt.title(\"R^2 test - Ridge\")\n",
    "plt.xlabel(\"Polynomial degree p\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.legend(title=\"Datapoints\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#plot R^2 train\n",
    "ns = sorted({n for (n, _) in results_ridge.keys()})\n",
    "plt.figure()\n",
    "for n_focus in ns:\n",
    "    ps = sorted({p for (n_key, p) in results_ridge.keys() if n_key == n_focus})\n",
    "    R2_tr_ridge = [results_ridge[(n_focus, p)][\"R2_train\"] for p in ps]\n",
    "    plt.plot(ps, R2_tr_ridge, marker=\"o\", label=f\"n={n_focus}\")\n",
    "plt.title(\"R^2 train - Ridge\")\n",
    "plt.xlabel(\"Polynomial degree p\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.legend(title=\"Datapoints\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#plot theta\n",
    "#m√• finne ut hva beste m√•te for det er \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "n_focus = ns[0]  # velg ett n\n",
    "\n",
    "plt.figure()\n",
    "for lam_focus in lams[:4]:  # f.eks. f√∏rste 4 Œª-verdier\n",
    "    ps_n = sorted(p for (n, p, model, lam) in results_ridge\n",
    "                  if n == n_focus and model == \"ridge\" and lam == lam_focus)\n",
    "    mse_te = [results_ridge[(n_focus, p, \"ridge\", lam_focus)][\"mse_test\"] for p in ps_n]\n",
    "    plt.plot(ps_n, mse_te, marker=\"o\", label=f\"Œª={lam_focus:g}\")\n",
    "plt.title(f\"Ridge ‚Äì Test MSE vs p (n={n_focus})\")\n",
    "plt.xlabel(\"Polynomial degree p\"); plt.ylabel(\"Test MSE\")\n",
    "plt.legend(title=\"Œª\"); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#chat\n",
    "\n",
    "# Plukk ut unike verdier av n, p, lam fra results_ridge\n",
    "Ns_ridge   = sorted({k[0] for k in results_ridge.keys()})\n",
    "Ps_ridge   = sorted({k[1] for k in results_ridge.keys()})\n",
    "LAMs_all   = sorted({k[3] for k in results_ridge.keys()})\n",
    "\n",
    "# Velg et ryddig delsett av lambdas for plotting over p\n",
    "# (juster gjerne lista hvis du har andre lam i lams)\n",
    "target_lams = [0.0, 1e-6, 1e-4, 1e-2, 1e0, 1e2]\n",
    "def nearest_lam(target, lam_list):\n",
    "    return min(lam_list, key=lambda z: abs(z - target))\n",
    "\n",
    "LAMs_subset = []\n",
    "for t in target_lams:\n",
    "    if len(LAMs_all) == 0: break\n",
    "    LAMs_subset.append(nearest_lam(t, LAMs_all))\n",
    "# fjern ev. duplikater\n",
    "LAMs_subset = sorted({float(x) for x in LAMs_subset})\n",
    "\n",
    "def get_ridge_metric(n, p, lam, metric):\n",
    "    return results_ridge[(n, p, \"ridge\", float(lam))][metric]\n",
    "\n",
    "def get_ols_metric(n, p, metric):\n",
    "    return results_ols[(n, p)][metric]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Sammenlign Ridge (utvalgte Œª) vs OLS: MSE_test og R¬≤_test som funksjon av p, for hver n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "for n_focus in Ns_ridge:\n",
    "    ps = sorted({k[1] for k in results_ridge.keys() if k[0] == n_focus})\n",
    "    \n",
    "    # --- Test MSE ---\n",
    "    plt.figure()\n",
    "    for lam in LAMs_subset:\n",
    "        mse_te = [get_ridge_metric(n_focus, p, lam, \"mse_test\") for p in ps]\n",
    "        lab = r\"$\\lambda={}$\".format(lam) if lam != 0 else r\"$\\lambda=0$ (‚âà OLS)\"\n",
    "        plt.plot(ps, mse_te, marker=\"o\", label=lab)\n",
    "    \n",
    "    # Legg p√• OLS (din del a) ) som svart, tykk linje\n",
    "    mse_te_ols = [get_ols_metric(n_focus, p, \"mse_test\") for p in ps]\n",
    "    plt.plot(ps, mse_te_ols, linewidth=3, label=\"OLS (del a)\", color=\"black\")\n",
    "    \n",
    "    plt.title(f\"Test MSE vs p ‚Äî Ridge (ulike Œª) og OLS, n={n_focus}\")\n",
    "    plt.xlabel(\"Polynomial degree p\"); plt.ylabel(\"Test MSE\")\n",
    "    plt.legend(title=\"Modell\"); plt.tight_layout(); plt.show()\n",
    "    \n",
    "    # --- Test R^2 ---\n",
    "    plt.figure()\n",
    "    for lam in LAMs_subset:\n",
    "        r2_te = [get_ridge_metric(n_focus, p, lam, \"R2_test\") for p in ps]\n",
    "        lab = r\"$\\lambda={}$\".format(lam) if lam != 0 else r\"$\\lambda=0$ (‚âà OLS)\"\n",
    "        plt.plot(ps, r2_te, marker=\"o\", label=lab)\n",
    "    \n",
    "    r2_te_ols = [get_ols_metric(n_focus, p, \"R2_test\") for p in ps]\n",
    "    plt.plot(ps, r2_te_ols, linewidth=3, label=\"OLS (del a)\", color=\"black\")\n",
    "    \n",
    "    plt.title(f\"Test $R^2$ vs p ‚Äî Ridge (ulike Œª) og OLS, n={n_focus}\")\n",
    "    plt.xlabel(\"Polynomial degree p\"); plt.ylabel(r\"$R^2$\")\n",
    "    plt.legend(title=\"Modell\"); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Stud√©r avhengighet av Œª direkte: MSE_test og R¬≤_test som funksjon av Œª (logskala), for noen utvalgte p (per n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "p_subset_candidates = [1, 3, 5, 10, 15]\n",
    "for n_focus in Ns_ridge:\n",
    "    ps_available = sorted({k[1] for k in results_ridge.keys() if k[0]==n_focus})\n",
    "    p_subset = [p for p in p_subset_candidates if p in ps_available]\n",
    "    if not p_subset: \n",
    "        continue\n",
    "    \n",
    "    # --- Test MSE vs lambda ---\n",
    "    plt.figure()\n",
    "    for p in p_subset:\n",
    "        lam_list = sorted({k[3] for k in results_ridge.keys() if k[0]==n_focus and k[1]==p})\n",
    "        mse_te = [get_ridge_metric(n_focus, p, lam, \"mse_test\") for lam in lam_list]\n",
    "        plt.semilogx(lam_list, mse_te, marker=\"o\", label=f\"p={p}\")\n",
    "    \n",
    "    plt.title(f\"Test MSE vs Œª ‚Äî Ridge, n={n_focus}\")\n",
    "    plt.xlabel(r\"$\\lambda$ (log)\"); plt.ylabel(\"Test MSE\")\n",
    "    plt.legend(title=\"Polynomgrad p\"); plt.tight_layout(); plt.show()\n",
    "    \n",
    "    # --- Test R^2 vs lambda ---\n",
    "    plt.figure()\n",
    "    for p in p_subset:\n",
    "        lam_list = sorted({k[3] for k in results_ridge.keys() if k[0]==n_focus and k[1]==p})\n",
    "        r2_te = [get_ridge_metric(n_focus, p, lam, \"R2_test\") for lam in lam_list]\n",
    "        plt.semilogx(lam_list, r2_te, marker=\"o\", label=f\"p={p}\")\n",
    "    \n",
    "    plt.title(f\"Test $R^2$ vs Œª ‚Äî Ridge, n={n_focus}\")\n",
    "    plt.xlabel(r\"$\\lambda$ (log)\"); plt.ylabel(r\"$R^2$\")\n",
    "    plt.legend(title=\"Polynomgrad p\"); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips til dr√∏fting (n√•r du ser figurene)\n",
    "\n",
    "OLS-kurven (Œª=0) vil ofte dyppe og s√• stige for test-MSE n√•r p blir stor (overtilpasning).\n",
    "\n",
    "For moderate/h√∏ye p vil en moderat Œª gi lavere test-MSE (bedre generalisering).\n",
    "\n",
    "R¬≤ speiler dette: h√∏yere R¬≤ for test ved passende Œª, s√¶rlig ved st√∏rre p.\n",
    "\n",
    "Heatmap gj√∏r det lett √• peke ut ‚Äúbeste omr√•de‚Äù (lav MSE) i (p, Œª)-planet for hvert n.\n",
    "\n",
    "Rop ut om du vil at jeg skal legge inn tilsvarende plott for train ogs√•, eller plotte ‚ÄñŒ∏‚Äñ (koeffisientnorm) mot p/Œª for √• vise stabilisering fra regul√¶risering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Part c: Writing your own gradient descent code}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def gd_fit(X, y, eta=0.1, num_iters=1000, lam=0.0, theta0=None, record_every=10):\n",
    "    \"\"\"\n",
    "    X: skalert design (n x p), uten intercept-kolonne\n",
    "    y: senterert target (n,)\n",
    "    eta: learning rate (fast)\n",
    "    num_iters: antall iterasjoner\n",
    "    lam: ridge-lambda (0 => OLS)\n",
    "    theta0: initialisering (p,). Hvis None: zeros.\n",
    "    record_every: hvor ofte vi lagrer MSE i historikk\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    theta = np.zeros(p) if theta0 is None else theta0.copy()\n",
    "\n",
    "    hist = {\"iter\": [], \"mse\": []}\n",
    "    for t in range(1, num_iters+1):\n",
    "        # grad for OLS: (2/n) X^T (XŒ∏ - y)\n",
    "        # Ridge: legg til 2ŒªŒ∏\n",
    "        residual = X @ theta - y\n",
    "        grad = (2.0/n) * (X.T @ residual) + 2.0 * lam * theta\n",
    "\n",
    "        theta -= eta * grad\n",
    "\n",
    "        if (t % record_every) == 0 or t == 1 or t == num_iters:\n",
    "            mse = np.mean(residual**2)\n",
    "            hist[\"iter\"].append(t)\n",
    "            hist[\"mse\"].append(mse)\n",
    "\n",
    "    return theta, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "\n",
    "num_iters = 2000 \n",
    "\n",
    "n_features = 2\n",
    "\n",
    "lam = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# OLS \n",
    "\n",
    "results_gd_ols = {}\n",
    "\n",
    "rng = np.random.default_rng(73) #random number generator med random seed = 73\n",
    "\n",
    "for n in datapoints:\n",
    "    \n",
    "    x = np.linspace(-1,1,n)\n",
    "    y = 1 / (1 + 25*x**2)\n",
    "    #noise = 0.5 * np.random.randn(n)    # Gaussian (normal distribution) noise\n",
    "    #y = y + noise\n",
    "\n",
    "    noise = 0.5 * rng.standard_normal(n)\n",
    "\n",
    "    y = y + noise \n",
    "\n",
    "    #Make one index-split per n, and reuse for all p\n",
    "    idx = np.arange(n)\n",
    "    idx_train, idx_test = train_test_split(idx, test_size=0.2, random_state=73)\n",
    "\n",
    "\n",
    "    for p in poly:\n",
    "        \n",
    "        Xp = polynomial_features(x, p, intercept=False)\n",
    "\n",
    "        #use the same indexes for Xp and y\n",
    "        X_train = Xp[idx_train, :]\n",
    "        X_test = Xp[idx_test, :]\n",
    "        y_train = y[idx_train]\n",
    "        y_test = y[idx_test]\n",
    "\n",
    "        #scale only on train, and then use the same scaler for test \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_s = scaler.transform(X_train)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "\n",
    "        #center y with train mean (and same offset on test)\n",
    "        y_offset = np.mean(y_train)\n",
    "        y_train_s = y_train - y_offset\n",
    "        y_test_s = y_test - y_offset\n",
    "\n",
    "            #for eta in etas:\n",
    "\n",
    "        # Gradient descent loop\n",
    "\n",
    "        theta_gdOLS = np.zeros(X_train_s.shape[1])\n",
    "\n",
    "        for t in range(num_iters):\n",
    "            # Compute gradients for OSL \n",
    "            grad_OLS = (2 / len(y_train_s)) * (X_train_s.T @ (X_train_s @ theta_gdOLS - y_train_s))\n",
    "            # Update parameters theta\n",
    "            theta_gdOLS = theta_gdOLS - eta * grad_OLS\n",
    "\n",
    "            #predicting y with the model (with sentered y)\n",
    "            y_hat_train_sc = X_train_s @ theta_gdOLS\n",
    "            y_hat_test_sc = X_test_s @ theta_gdOLS\n",
    "\n",
    "            #unnormalising y - to be on the same scale as the original y values \n",
    "            #y_hat_train = y_hat_train_sc + y_offset\n",
    "            #y_hat_test = y_hat_test_sc + y_offset\n",
    "\n",
    "            #calculate MSE \n",
    "            mse_train = np.mean((y_train_s - y_hat_train_sc)**2)\n",
    "            mse_test = np.mean((y_test_s - y_hat_test_sc)**2)\n",
    "\n",
    "            #calculate R^2\n",
    "            SSE_tr = np.sum((y_train_s - y_hat_train_sc)**2)\n",
    "            SST_tr = np.sum((y_train_s - y_train_s.mean())**2)\n",
    "            #SST_tr = np.sum(y_train_s**2)\n",
    "            R2_train = 1 - SSE_tr / SST_tr\n",
    "\n",
    "            SSE_te = np.sum((y_test_s - y_hat_test_sc)**2)\n",
    "            SST_te = np.sum((y_test_s - y_train_s.mean())**2)\n",
    "            #SST_te = np.sum(y_test_s**2)\n",
    "            R2_test = 1 - SSE_te / SST_te\n",
    "\n",
    "        results_gd_ols[(n, p)] = {\n",
    "            #train, test, scaler values\n",
    "            \"X_train\": X_train_s, \"X_test\": X_test_s,\n",
    "            \"y_train\": y_train_s, \"y_test\": y_test_s,\n",
    "            \"scaler_mean\": scaler.mean_, \"scaler_scale\": scaler.scale_,\n",
    "            \"y_offset\": y_offset,\n",
    "            #index and x values\n",
    "            \"idx_train\": idx_train, \"idx_test\": idx_test,\n",
    "            \"x_train\": x[idx_train], \"x_test\": x[idx_test],\n",
    "            #theta, mse and R^2 values\n",
    "            \"theta\": theta_gdOLS, \n",
    "            \"mse_train\": mse_train, \"mse_test\": mse_test,\n",
    "            \"R2_train\": R2_train, \"R2_test\": R2_test, \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#kode for plotting fra chatgpt \n",
    "\n",
    "# Sammenlign Test MSE for GD vs OLS\n",
    "ns = sorted({n for (n, _) in results_gd_ols.keys()})\n",
    "plt.figure()\n",
    "for n_focus in ns:\n",
    "    ps = sorted({p for (n_key, p) in results_gd_ols.keys() if n_key == n_focus})\n",
    "    \n",
    "    mse_te_gd = [results_gd_ols[(n_focus, p)][\"mse_test\"] for p in ps]\n",
    "    mse_te_ols = [results_ols[(n_focus, p)][\"mse_test\"] for p in ps]\n",
    "    \n",
    "    plt.plot(ps, mse_te_gd, marker=\"o\", label=f\"GD n={n_focus}\")\n",
    "    plt.plot(ps, mse_te_ols, linestyle=\"--\", label=f\"OLS n={n_focus}\")\n",
    "    \n",
    "plt.title(\"Test MSE ‚Äî Gradient Descent vs OLS\")\n",
    "plt.xlabel(\"Polynomial degree p\")\n",
    "plt.ylabel(\"Test MSE\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eksempel for R^2 test\n",
    "plt.figure()\n",
    "for n_focus in ns:\n",
    "    ps = sorted({p for (n_key, p) in results_gd_ols.keys() if n_key == n_focus})\n",
    "    \n",
    "    r2_te_gd = [results_gd_ols[(n_focus, p)][\"R2_test\"] for p in ps]\n",
    "    r2_te_ols = [results_ols[(n_focus, p)][\"R2_test\"] for p in ps]\n",
    "    \n",
    "    plt.plot(ps, r2_te_gd, marker=\"o\", label=f\"GD n={n_focus}\")\n",
    "    plt.plot(ps, r2_te_ols, linestyle=\"--\", label=f\"OLS n={n_focus}\")\n",
    "    \n",
    "plt.title(\"Test $R^2$ ‚Äî Gradient Descent vs OLS\")\n",
    "plt.xlabel(\"Polynomial degree p\")\n",
    "plt.ylabel(\"$R^2$\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "\n",
    "results_gd_ridge = {}\n",
    "\n",
    "rng = np.random.default_rng(73) #random number generator med random seed = 73\n",
    "\n",
    "for n in datapoints:\n",
    "    \n",
    "    x = np.linspace(-1,1,n)\n",
    "    y = 1 / (1 + 25*x**2)\n",
    "    #noise = 0.5 * np.random.randn(n)    # Gaussian (normal distribution) noise\n",
    "    #y = y + noise\n",
    "\n",
    "    noise = 0.5 * rng.standard_normal(n)\n",
    "\n",
    "    y = y + noise \n",
    "\n",
    "    #Make one index-split per n, and reuse for all p\n",
    "    idx = np.arange(n)\n",
    "    idx_train, idx_test = train_test_split(idx, test_size=0.2, random_state=73)\n",
    "\n",
    "\n",
    "    for p in poly:\n",
    "        \n",
    "        Xp = polynomial_features(x, p, intercept=False)\n",
    "\n",
    "        #use the same indexes for Xp and y\n",
    "        X_train = Xp[idx_train, :]\n",
    "        X_test = Xp[idx_test, :]\n",
    "        y_train = y[idx_train]\n",
    "        y_test = y[idx_test]\n",
    "\n",
    "        #scale only on train, and then use the same scaler for test \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_s = scaler.transform(X_train)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "\n",
    "        #center y with train mean (and same offset on test)\n",
    "        y_offset = np.mean(y_train)\n",
    "        y_train_s = y_train - y_offset\n",
    "        y_test_s = y_test - y_offset\n",
    "\n",
    "            #for eta in etas:\n",
    "\n",
    "        # Gradient descent loop\n",
    "\n",
    "        theta_gd_ridge = np.zeros(X_train_s.shape[1])\n",
    "\n",
    "        for t in range(num_iters):\n",
    "            # Compute gradients for ridge\n",
    "            grad_Ridge = (2 / len(y_train_s)) * (X_train_s.T @ (X_train_s @ theta_gd_ridge - y_train_s)) + 2 * lam * theta_gd_ridge\n",
    "\n",
    "            # Update parameters theta\n",
    "            theta_gd_ridge = theta_gd_ridge - eta * grad_Ridge\n",
    "\n",
    "            #predicting y with the model (with sentered y)\n",
    "            y_hat_train_sc = X_train_s @ theta_gd_ridge\n",
    "            y_hat_test_sc = X_test_s @ theta_gd_ridge\n",
    "\n",
    "            #unnormalising y - to be on the same scale as the original y values \n",
    "            #y_hat_train = y_hat_train_sc + y_offset\n",
    "            #y_hat_test = y_hat_test_sc + y_offset\n",
    "\n",
    "            #calculate MSE \n",
    "            mse_train = np.mean((y_train_s - y_hat_train_sc)**2)\n",
    "            mse_test = np.mean((y_test_s - y_hat_test_sc)**2)\n",
    "\n",
    "            #calculate R^2\n",
    "            SSE_tr = np.sum((y_train_s - y_hat_train_sc)**2)\n",
    "            SST_tr = np.sum((y_train_s - y_train_s.mean())**2)\n",
    "            #SST_tr = np.sum(y_train_s**2)\n",
    "            R2_train = 1 - SSE_tr / SST_tr\n",
    "\n",
    "            SSE_te = np.sum((y_test_s - y_hat_test_sc)**2)\n",
    "            SST_te = np.sum((y_test_s - y_train_s.mean())**2)\n",
    "            #SST_te = np.sum(y_test_s**2)\n",
    "            R2_test = 1 - SSE_te / SST_te\n",
    "\n",
    "         # ---------- LAGRE PER (n, p, modell, lam) ----------\n",
    "        key = (n, p, \"ridge\", float(lam))\n",
    "        results_gd_ridge[key] = {\n",
    "            \"theta\": theta_gd_ridge,\n",
    "            \"theta_norm\": theta_norm,\n",
    "            \"mse_train\": mse_train, \"mse_test\": mse_test,\n",
    "            \"R2_train\": R2_train,   \"R2_test\": R2_test,\n",
    "            \"lam\": float(lam), \"reg_label\": \"ridge\",\n",
    "            \"scaler_mean\": scaler.mean_, \"scaler_scale\": scaler.scale_,\n",
    "            \"y_offset\": y_offset,\n",
    "            \"idx_train\": idx_train, \"idx_test\": idx_test,\n",
    "            \"x_train\": x[idx_train], \"x_test\": x[idx_test],\n",
    "        }\n",
    "\"\"\"\n",
    "        results_gd_ridge[(n, p)] = {\n",
    "            #train, test, scaler values\n",
    "            \"X_train\": X_train_s, \"X_test\": X_test_s,\n",
    "            \"y_train\": y_train_s, \"y_test\": y_test_s,\n",
    "            \"scaler_mean\": scaler.mean_, \"scaler_scale\": scaler.scale_,\n",
    "            \"y_offset\": y_offset,\n",
    "            #index and x values\n",
    "            \"idx_train\": idx_train, \"idx_test\": idx_test,\n",
    "            \"x_train\": x[idx_train], \"x_test\": x[idx_test],\n",
    "            #theta, mse and R^2 values\n",
    "            \"theta\": theta_gd_ridge, \n",
    "            \"mse_train\": mse_train, \"mse_test\": mse_test,\n",
    "            \"R2_train\": R2_train, \"R2_test\": R2_test, \n",
    "        }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "for n_focus in ns:\n",
    "    plt.figure()\n",
    "    ps = sorted({p for (n_key, p) in results_ols.keys() if n_key == n_focus})\n",
    "\n",
    "    mse_te_ols = [results_ols[(n_focus, p)][\"mse_test\"] for p in ps]\n",
    "    plt.plot(ps, mse_te_ols, \"--\", color=\"black\", label=\"OLS\")\n",
    "\n",
    "    mse_te_ridge = []\n",
    "    for p in ps:\n",
    "        lam_match = nearest_lambda(lam, n_focus, p)\n",
    "        mse_te_ridge.append(results_ridge[(n_focus, p, \"ridge\", lam_match)][\"mse_test\"])\n",
    "    plt.plot(ps, mse_te_ridge, \":\", color=\"blue\", label=\"Ridge (analytisk)\")\n",
    "\n",
    "    mse_te_gd = [results_gd_ridge[(n_focus, p, \"ridge\", float(lam))][\"mse_test\"] for p in ps]\n",
    "    plt.plot(ps, mse_te_gd, \"o-\", color=\"red\", label=\"Ridge (GD)\")\n",
    "\n",
    "    plt.title(f\"Test MSE ‚Äî n={n_focus}, Œª={lam}\")\n",
    "    plt.xlabel(\"Polynomial degree p\")\n",
    "    plt.ylabel(\"Test MSE\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "\n",
    "for ax, n_focus in zip(axes.ravel(), ns):\n",
    "    ps = sorted({p for (n_key, p) in results_ols.keys() if n_key == n_focus})\n",
    "\n",
    "    mse_te_ols = [results_ols[(n_focus, p)][\"mse_test\"] for p in ps]\n",
    "    ax.plot(ps, mse_te_ols, \"--\", color=\"black\", label=\"OLS\")\n",
    "\n",
    "    mse_te_ridge = []\n",
    "    for p in ps:\n",
    "        lam_match = nearest_lambda(lam, n_focus, p)\n",
    "        mse_te_ridge.append(results_ridge[(n_focus, p, \"ridge\", lam_match)][\"mse_test\"])\n",
    "    ax.plot(ps, mse_te_ridge, \":\", color=\"blue\", label=\"Ridge (analytisk)\")\n",
    "\n",
    "    mse_te_gd = [results_gd_ridge[(n_focus, p, \"ridge\", float(lam))][\"mse_test\"] for p in ps]\n",
    "    ax.plot(ps, mse_te_gd, \"o-\", color=\"red\", label=\"Ridge (GD)\")\n",
    "\n",
    "    ax.set_title(f\"n={n_focus}\")\n",
    "    ax.set_xlabel(\"Polynomial degree p\")\n",
    "    ax.set_ylabel(\"Test MSE\")\n",
    "\n",
    "# Felles legend\n",
    "handles, labels = axes[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper center\", ncol=3)\n",
    "fig.suptitle(f\"Test MSE ‚Äî OLS vs Ridge (analytisk og GD), Œª={lam}\")\n",
    "plt.tight_layout(rect=[0,0,1,0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Part d: Including momentum and more advanced ways to update the learning the rate}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, eta=0.01, num_iters=1000, lam=0.0, method=\"vanilla\"):\n",
    "    n, p = X.shape\n",
    "    theta = np.zeros(p)\n",
    "    eps = 1e-8\n",
    "\n",
    "    # init tilleggsvariabler\n",
    "    v = np.zeros(p)        # for momentum\n",
    "    G = np.zeros(p)        # for AdaGrad\n",
    "    Eg2 = np.zeros(p)      # for RMSprop\n",
    "    m = np.zeros(p)        # for Adam\n",
    "    v_hat = np.zeros(p)    # for Adam\n",
    "\n",
    "    beta = 0.9\n",
    "    beta1, beta2 = 0.9, 0.999\n",
    "\n",
    "    hist = {\"iter\": [], \"mse_train\": []}\n",
    "\n",
    "    for t in range(1, num_iters+1):\n",
    "        grad = (2/n) * (X.T @ (X @ theta - y)) + 2*lam*theta\n",
    "\n",
    "        if method == \"vanilla\":   # vanlig GD\n",
    "            theta -= eta * grad\n",
    "\n",
    "        elif method == \"momentum\":\n",
    "            v = beta*v + eta*grad\n",
    "            theta -= v\n",
    "\n",
    "        elif method == \"adagrad\":\n",
    "            G += grad**2\n",
    "            theta -= (eta / (np.sqrt(G)+eps)) * grad\n",
    "\n",
    "        elif method == \"rmsprop\":\n",
    "            Eg2 = beta*Eg2 + (1-beta)*grad**2\n",
    "            theta -= (eta / (np.sqrt(Eg2)+eps)) * grad\n",
    "\n",
    "        elif method == \"adam\":\n",
    "            m = beta1*m + (1-beta1)*grad\n",
    "            v_hat = beta2*v_hat + (1-beta2)*(grad**2)\n",
    "            m_corr = m / (1-beta1**t)\n",
    "            v_corr = v_hat / (1-beta2**t)\n",
    "            theta -= (eta / (np.sqrt(v_corr)+eps)) * m_corr\n",
    "\n",
    "        # logg MSE underveis (valgfritt)\n",
    "        if t % 10 == 0:\n",
    "            mse_train = np.mean((y - X@theta)**2)\n",
    "            hist[\"iter\"].append(t)\n",
    "            hist[\"mse_train\"].append(mse_train)\n",
    "\n",
    "    return theta, hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- Velg case -----\n",
    "n_pick, p_pick = 1000, 5         # endre hvis du vil se andre\n",
    "lam_ridge = 0.1                  # ridge-Œª i del d\n",
    "eta = 0.1                        # base lr\n",
    "num_iters = 2000\n",
    "eps = 1e-8\n",
    "\n",
    "# Hent ferdigskalert data fra results_ols (din del a lagrer X_train/X_test/y_train/y_test senter/skalert)\n",
    "Xtr = results_ols[(n_pick, p_pick)][\"X_train\"]\n",
    "Xte = results_ols[(n_pick, p_pick)][\"X_test\"]\n",
    "ytr = results_ols[(n_pick, p_pick)][\"y_train\"]\n",
    "yte = results_ols[(n_pick, p_pick)][\"y_test\"]\n",
    "\n",
    "def gd_run(X, y, X_val, y_val, eta=0.1, num_iters=2000, lam=0.0, method=\"vanilla\",\n",
    "           beta=0.9, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "    n, p = X.shape\n",
    "    theta = np.zeros(p)\n",
    "\n",
    "    # Buffere for ulike metoder\n",
    "    v = np.zeros(p)      # momentum\n",
    "    G = np.zeros(p)      # AdaGrad (akkumulert grad^2)\n",
    "    Eg2 = np.zeros(p)    # RMSprop (EMA av grad^2)\n",
    "    m = np.zeros(p)      # Adam m_t\n",
    "    v_adam = np.zeros(p) # Adam v_t\n",
    "\n",
    "    hist = {\"iter\": [], \"mse_train\": [], \"mse_test\": []}\n",
    "\n",
    "    for t in range(1, num_iters+1):\n",
    "        # felles gradient for OLS/Ridge (p√• skalert X og senterert y)\n",
    "        grad = (2.0/n) * (X.T @ (X @ theta - y)) + 2.0 * lam * theta\n",
    "\n",
    "        if method == \"vanilla\":\n",
    "            theta -= eta * grad\n",
    "\n",
    "        elif method == \"momentum\":\n",
    "            v = beta * v + eta * grad\n",
    "            theta -= v\n",
    "\n",
    "        elif method == \"adagrad\":\n",
    "            G += grad**2\n",
    "            theta -= (eta / (np.sqrt(G) + eps)) * grad\n",
    "\n",
    "        elif method == \"rmsprop\":\n",
    "            Eg2 = beta * Eg2 + (1.0 - beta) * (grad**2)\n",
    "            theta -= (eta / (np.sqrt(Eg2) + eps)) * grad\n",
    "\n",
    "        elif method == \"adam\":\n",
    "            m = beta1 * m + (1.0 - beta1) * grad\n",
    "            v_adam = beta2 * v_adam + (1.0 - beta2) * (grad**2)\n",
    "            m_hat = m / (1.0 - beta1**t)\n",
    "            v_hat = v_adam / (1.0 - beta2**t)\n",
    "            theta -= (eta / (np.sqrt(v_hat) + eps)) * m_hat\n",
    "\n",
    "        # logg (hver 10. iterasjon for ryddigere kurver)\n",
    "        if t % 10 == 0 or t == 1 or t == num_iters:\n",
    "            tr_pred = X @ theta\n",
    "            te_pred = X_val @ theta\n",
    "            mse_tr = np.mean((y - tr_pred)**2)\n",
    "            mse_te = np.mean((y_val - te_pred)**2)\n",
    "            hist[\"iter\"].append(t)\n",
    "            hist[\"mse_train\"].append(mse_tr)\n",
    "            hist[\"mse_test\"].append(mse_te)\n",
    "\n",
    "    return theta, hist\n",
    "\n",
    "methods = [\"vanilla\", \"momentum\", \"adagrad\", \"rmsprop\", \"adam\"]\n",
    "\n",
    "# ----- Kj√∏r OLS (Œª=0) -----\n",
    "hists_ols = {}\n",
    "for m in methods:\n",
    "    _, h = gd_run(Xtr, ytr, Xte, yte, eta=eta, num_iters=num_iters, lam=0.0, method=m)\n",
    "    hists_ols[m] = h\n",
    "\n",
    "# ----- Kj√∏r Ridge (Œª=lam_ridge) -----\n",
    "hists_ridge = {}\n",
    "for m in methods:\n",
    "    _, h = gd_run(Xtr, ytr, Xte, yte, eta=eta, num_iters=num_iters, lam=lam_ridge, method=m)\n",
    "    hists_ridge[m] = h\n",
    "\n",
    "# ----- Plot: OLS l√¶ringskurver -----\n",
    "plt.figure(figsize=(7,5))\n",
    "for m in methods:\n",
    "    plt.plot(hists_ols[m][\"iter\"], hists_ols[m][\"mse_train\"], label=f\"{m} (train)\")\n",
    "    plt.plot(hists_ols[m][\"iter\"], hists_ols[m][\"mse_test\"], linestyle=\"--\", label=f\"{m} (test)\")\n",
    "plt.title(f\"OLS (Œª=0) ‚Äì Learning curves, n={n_pick}, p={p_pick}\")\n",
    "plt.xlabel(\"Iterations\"); plt.ylabel(\"MSE\")\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\"); plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----- Plot: Ridge l√¶ringskurver -----\n",
    "plt.figure(figsize=(7,5))\n",
    "for m in methods:\n",
    "    plt.plot(hists_ridge[m][\"iter\"], hists_ridge[m][\"mse_train\"], label=f\"{m} (train)\")\n",
    "    plt.plot(hists_ridge[m][\"iter\"], hists_ridge[m][\"mse_test\"], linestyle=\"--\", label=f\"{m} (test)\")\n",
    "plt.title(f\"Ridge (Œª={lam_ridge}) ‚Äì Learning curves, n={n_pick}, p={p_pick}\")\n",
    "plt.xlabel(\"Iterations\"); plt.ylabel(\"MSE\")\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\"); plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "num_iters = 2000\n",
    "methods = [\"vanilla\", \"momentum\", \"adagrad\", \"rmsprop\", \"adam\"]\n",
    "\n",
    "def run_and_plot_learning_curves(n_pick, p_pick, lam_val=0.0, title_prefix=\"OLS\"):\n",
    "    # Hent skalert X/y fra results_ols\n",
    "    Xtr = results_ols[(n_pick, p_pick)][\"X_train\"]\n",
    "    Xte = results_ols[(n_pick, p_pick)][\"X_test\"]\n",
    "    ytr = results_ols[(n_pick, p_pick)][\"y_train\"]\n",
    "    yte = results_ols[(n_pick, p_pick)][\"y_test\"]\n",
    "\n",
    "    # Tren alle metoder\n",
    "    hists = {}\n",
    "    for m in methods:\n",
    "        _, h = gd_run(Xtr, ytr, Xte, yte, eta=eta, num_iters=num_iters, lam=lam_val, method=m)\n",
    "        hists[m] = h\n",
    "\n",
    "    # Plot log-skala (train og test)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    for m in methods:\n",
    "        plt.plot(hists[m][\"iter\"], hists[m][\"mse_train\"], label=f\"{m} (train)\")\n",
    "        plt.plot(hists[m][\"iter\"], hists[m][\"mse_test\"], linestyle=\"--\", label=f\"{m} (test)\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Iterations\"); plt.ylabel(\"MSE (log)\")\n",
    "    plt.title(f\"{title_prefix} (Œª={lam_val}) ‚Äì Learning curves, n={n_pick}, p={p_pick}\")\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Zoom p√• tidlig fase (f√∏rste 200 iterasjoner)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    for m in methods:\n",
    "        it = np.array(hists[m][\"iter\"])\n",
    "        mtr = np.array(hists[m][\"mse_train\"])\n",
    "        mte = np.array(hists[m][\"mse_test\"])\n",
    "        mask = it <= 200\n",
    "        plt.plot(it[mask], mtr[mask], label=f\"{m} (train)\")\n",
    "        plt.plot(it[mask], mte[mask], linestyle=\"--\", label=f\"{m} (test)\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Iterations (‚â§ 200)\"); plt.ylabel(\"MSE (log)\")\n",
    "    plt.title(f\"{title_prefix} (Œª={lam_val}) ‚Äì Early learning, n={n_pick}, p={p_pick}\")\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Kj√∏r for p=5 og p=15 (samme n)\n",
    "n_pick = 1000\n",
    "run_and_plot_learning_curves(n_pick, p_pick=5,  lam_val=0.0, title_prefix=\"OLS\")\n",
    "run_and_plot_learning_curves(n_pick, p_pick=15, lam_val=0.0, title_prefix=\"OLS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "lam_ridge = 0.1\n",
    "n_pick = 1000\n",
    "run_and_plot_learning_curves(n_pick, p_pick=5,  lam_val=lam_ridge, title_prefix=\"Ridge\")\n",
    "run_and_plot_learning_curves(n_pick, p_pick=15, lam_val=lam_ridge, title_prefix=\"Ridge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
